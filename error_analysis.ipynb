{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sohamchatterjee/miniforge3/envs/DL2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some samples - Test split of SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise_text = \"This church choir sings to the masses as they sing joyous songs from the book at a church.\"\n",
    "hypothesis_text = \"The church has cracks in the ceiling.\"\n",
    "label = 'neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline - Load model from pre-trained checkpoint and analyse on some samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold label - neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: entailment\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_idx = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/word_to_idx.pt\")\n",
    "pad_idx = word_to_idx.get('<pad>', 0)\n",
    "unk_idx = word_to_idx.get('<unk>', 1)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def encode(tokens):\n",
    "    return [word_to_idx.get(t, unk_idx) for t in tokens]\n",
    "\n",
    "\n",
    "premise_tokens = preprocess_text(premise_text)\n",
    "hypothesis_tokens = preprocess_text(hypothesis_text)\n",
    "premise_encoded = torch.tensor(encode(premise_tokens))\n",
    "hypothesis_encoded = torch.tensor(encode(hypothesis_tokens))\n",
    "\n",
    "premise_padded = pad_sequence([premise_encoded], batch_first=True, padding_value=pad_idx)\n",
    "hypothesis_padded = pad_sequence([hypothesis_encoded], batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "embedding_matrix = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/embedding_matrix.pt\", weights_only=False)\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True, padding_idx=pad_idx)\n",
    "\n",
    "def combine(u, v):\n",
    "    return torch.cat([u, v, torch.abs(u - v), u * v], dim=1)\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        mask = (x != pad_idx).unsqueeze(-1).float()\n",
    "        summed = torch.sum(embedded * mask, dim=1)\n",
    "        lengths = torch.sum(mask, dim=1)\n",
    "        return summed / lengths.clamp(min=1)\n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, hidden_dim=512, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(4 * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        u = self.encoder(premise)\n",
    "        v = self.encoder(hypothesis)\n",
    "        combined = combine(u, v)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "hidden_dim = 300\n",
    "model = NLIClassifier(SentenceEncoder(embedding_layer), input_dim=hidden_dim)\n",
    "model.load_state_dict(torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/Baseline/snli_baseline_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(premise_padded, hypothesis_padded)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "print(f\"Prediction: {label_map[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: contradiction\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_idx = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/word_to_idx.pt\")\n",
    "pad_idx = word_to_idx.get('<pad>', 0)\n",
    "unk_idx = word_to_idx.get('<unk>', 1)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def encode(tokens):\n",
    "    return [word_to_idx.get(t, unk_idx) for t in tokens]\n",
    "\n",
    "\n",
    "premise_tokens = preprocess_text(premise_text)\n",
    "hypothesis_tokens = preprocess_text(hypothesis_text)\n",
    "premise_encoded = torch.tensor(encode(premise_tokens))\n",
    "hypothesis_encoded = torch.tensor(encode(hypothesis_tokens))\n",
    "\n",
    "\n",
    "premise_padded = pad_sequence([premise_encoded], batch_first=True, padding_value=pad_idx)\n",
    "hypothesis_padded = pad_sequence([hypothesis_encoded], batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "embedding_matrix = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/embedding_matrix.pt\", weights_only=False)\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True, padding_idx=pad_idx)\n",
    "\n",
    "def combine(u, v):\n",
    "    return torch.cat([u, v, torch.abs(u - v), u * v], dim=1)\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim=300):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_layer.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  \n",
    "        lengths = (x != 0).sum(dim=1).clamp(min=1).cpu()  \n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        _, (h_n, _) = self.lstm(packed)  \n",
    "        return h_n.squeeze(0) \n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, hidden_dim=512, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(4 * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        u = self.encoder(premise)\n",
    "        v = self.encoder(hypothesis)\n",
    "        combined = combine(u, v)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "hidden_dim = 300\n",
    "model = NLIClassifier(SentenceEncoder(embedding_layer), input_dim=hidden_dim)\n",
    "model.load_state_dict(torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/v1/snli_v1_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(premise_padded, hypothesis_padded)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "print(f\"Prediction: {label_map[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidirectional LSTM wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: contradiction\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_idx = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/word_to_idx.pt\")\n",
    "pad_idx = word_to_idx.get('<pad>', 0)\n",
    "unk_idx = word_to_idx.get('<unk>', 1)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def encode(tokens):\n",
    "    return [word_to_idx.get(t, unk_idx) for t in tokens]\n",
    "\n",
    "\n",
    "premise_tokens = preprocess_text(premise_text)\n",
    "hypothesis_tokens = preprocess_text(hypothesis_text)\n",
    "premise_encoded = torch.tensor(encode(premise_tokens))\n",
    "hypothesis_encoded = torch.tensor(encode(hypothesis_tokens))\n",
    "\n",
    "\n",
    "premise_padded = pad_sequence([premise_encoded], batch_first=True, padding_value=pad_idx)\n",
    "hypothesis_padded = pad_sequence([hypothesis_encoded], batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "embedding_matrix = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/embedding_matrix.pt\", weights_only=False)\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True, padding_idx=pad_idx)\n",
    "\n",
    "def combine(u, v):\n",
    "    return torch.cat([u, v, torch.abs(u - v), u * v], dim=1)\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim=300):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_layer.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  \n",
    "        lengths = (x != 0).sum(dim=1).clamp(min=1).cpu()\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        _, (h_n, _) = self.lstm(packed)  \n",
    "\n",
    "        \n",
    "        h_forward = h_n[0]  \n",
    "        h_backward = h_n[1]  \n",
    "        sentence_rep = torch.cat([h_forward, h_backward], dim=1)  \n",
    "        return sentence_rep\n",
    "\n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, hidden_dim=512, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(4 * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        u = self.encoder(premise)\n",
    "        v = self.encoder(hypothesis)\n",
    "        combined = combine(u, v)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "hidden_dim = 300\n",
    "model = NLIClassifier(SentenceEncoder(embedding_layer, hidden_dim), input_dim=2 * hidden_dim)\n",
    "model.load_state_dict(torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/v2/snli_v2_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(premise_padded, hypothesis_padded)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "print(f\"Prediction: {label_map[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi LSTM wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM with max pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: contradiction\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_idx = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/word_to_idx.pt\")\n",
    "pad_idx = word_to_idx.get('<pad>', 0)\n",
    "unk_idx = word_to_idx.get('<unk>', 1)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def encode(tokens):\n",
    "    return [word_to_idx.get(t, unk_idx) for t in tokens]\n",
    "\n",
    "\n",
    "premise_tokens = preprocess_text(premise_text)\n",
    "hypothesis_tokens = preprocess_text(hypothesis_text)\n",
    "premise_encoded = torch.tensor(encode(premise_tokens))\n",
    "hypothesis_encoded = torch.tensor(encode(hypothesis_tokens))\n",
    "\n",
    "\n",
    "premise_padded = pad_sequence([premise_encoded], batch_first=True, padding_value=pad_idx)\n",
    "hypothesis_padded = pad_sequence([hypothesis_encoded], batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "embedding_matrix = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/embedding_matrix.pt\", weights_only=False)\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True, padding_idx=pad_idx)\n",
    "\n",
    "def combine(u, v):\n",
    "    return torch.cat([u, v, torch.abs(u - v), u * v], dim=1)\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim=300):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_layer.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  \n",
    "        lengths = (x != 0).sum(dim=1).clamp(min=1).cpu()\n",
    "\n",
    "        \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        \n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        sentence_rep, _ = torch.max(lstm_out, dim=1)  \n",
    "        return sentence_rep\n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, hidden_dim=512, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(4 * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        u = self.encoder(premise)\n",
    "        v = self.encoder(hypothesis)\n",
    "        combined = combine(u, v)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "hidden_dim = 300\n",
    "model = NLIClassifier(SentenceEncoder(embedding_layer, hidden_dim), input_dim=2 * hidden_dim)\n",
    "model.load_state_dict(torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/v3/snli_v3_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(premise_padded, hypothesis_padded)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "print(f\"Prediction: {label_map[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM with max pooling wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All 4 models are wrong on this example. This might be because this is a hard sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some specific examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise_text = \"Two men sitting in the sun\"\n",
    "hypothesis_text = \"Nobody is sitting in the shade\"\n",
    "label = 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Prediction: entailment\n",
      "Uni LSTM Prediction: contradiction\n",
      "BiLSTM Prediction: contradiction\n",
      "Bi LSTM with max poolong Prediction: contradiction\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_idx = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/word_to_idx.pt\")\n",
    "pad_idx = word_to_idx.get('<pad>', 0)\n",
    "unk_idx = word_to_idx.get('<unk>', 1)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def encode(tokens):\n",
    "    return [word_to_idx.get(t, unk_idx) for t in tokens]\n",
    "\n",
    "\n",
    "premise_tokens = preprocess_text(premise_text)\n",
    "hypothesis_tokens = preprocess_text(hypothesis_text)\n",
    "premise_encoded = torch.tensor(encode(premise_tokens))\n",
    "hypothesis_encoded = torch.tensor(encode(hypothesis_tokens))\n",
    "\n",
    "premise_padded = pad_sequence([premise_encoded], batch_first=True, padding_value=pad_idx)\n",
    "hypothesis_padded = pad_sequence([hypothesis_encoded], batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "embedding_matrix = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/embedding_matrix.pt\", weights_only=False)\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True, padding_idx=pad_idx)\n",
    "\n",
    "def combine(u, v):\n",
    "    return torch.cat([u, v, torch.abs(u - v), u * v], dim=1)\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        mask = (x != pad_idx).unsqueeze(-1).float()\n",
    "        summed = torch.sum(embedded * mask, dim=1)\n",
    "        lengths = torch.sum(mask, dim=1)\n",
    "        return summed / lengths.clamp(min=1)\n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, hidden_dim=512, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(4 * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        u = self.encoder(premise)\n",
    "        v = self.encoder(hypothesis)\n",
    "        combined = combine(u, v)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "hidden_dim = 300\n",
    "model = NLIClassifier(SentenceEncoder(embedding_layer), input_dim=hidden_dim)\n",
    "model.load_state_dict(torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/Baseline/snli_baseline_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(premise_padded, hypothesis_padded)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "print(f\"Baseline Prediction: {label_map[predicted_class]}\")\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_idx = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/word_to_idx.pt\")\n",
    "pad_idx = word_to_idx.get('<pad>', 0)\n",
    "unk_idx = word_to_idx.get('<unk>', 1)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def encode(tokens):\n",
    "    return [word_to_idx.get(t, unk_idx) for t in tokens]\n",
    "\n",
    "\n",
    "premise_tokens = preprocess_text(premise_text)\n",
    "hypothesis_tokens = preprocess_text(hypothesis_text)\n",
    "premise_encoded = torch.tensor(encode(premise_tokens))\n",
    "hypothesis_encoded = torch.tensor(encode(hypothesis_tokens))\n",
    "\n",
    "\n",
    "premise_padded = pad_sequence([premise_encoded], batch_first=True, padding_value=pad_idx)\n",
    "hypothesis_padded = pad_sequence([hypothesis_encoded], batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "embedding_matrix = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/embedding_matrix.pt\", weights_only=False)\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True, padding_idx=pad_idx)\n",
    "\n",
    "def combine(u, v):\n",
    "    return torch.cat([u, v, torch.abs(u - v), u * v], dim=1)\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim=300):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_layer.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  \n",
    "        lengths = (x != 0).sum(dim=1).clamp(min=1).cpu()  \n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        _, (h_n, _) = self.lstm(packed)  \n",
    "        return h_n.squeeze(0) \n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, hidden_dim=512, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(4 * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        u = self.encoder(premise)\n",
    "        v = self.encoder(hypothesis)\n",
    "        combined = combine(u, v)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "hidden_dim = 300\n",
    "model = NLIClassifier(SentenceEncoder(embedding_layer), input_dim=hidden_dim)\n",
    "model.load_state_dict(torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/v1/snli_v1_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(premise_padded, hypothesis_padded)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "print(f\"Uni LSTM Prediction: {label_map[predicted_class]}\")\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_idx = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/word_to_idx.pt\")\n",
    "pad_idx = word_to_idx.get('<pad>', 0)\n",
    "unk_idx = word_to_idx.get('<unk>', 1)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def encode(tokens):\n",
    "    return [word_to_idx.get(t, unk_idx) for t in tokens]\n",
    "\n",
    "\n",
    "premise_tokens = preprocess_text(premise_text)\n",
    "hypothesis_tokens = preprocess_text(hypothesis_text)\n",
    "premise_encoded = torch.tensor(encode(premise_tokens))\n",
    "hypothesis_encoded = torch.tensor(encode(hypothesis_tokens))\n",
    "\n",
    "\n",
    "premise_padded = pad_sequence([premise_encoded], batch_first=True, padding_value=pad_idx)\n",
    "hypothesis_padded = pad_sequence([hypothesis_encoded], batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "embedding_matrix = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/embedding_matrix.pt\", weights_only=False)\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True, padding_idx=pad_idx)\n",
    "\n",
    "def combine(u, v):\n",
    "    return torch.cat([u, v, torch.abs(u - v), u * v], dim=1)\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim=300):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_layer.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  \n",
    "        lengths = (x != 0).sum(dim=1).clamp(min=1).cpu()\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        _, (h_n, _) = self.lstm(packed)  \n",
    "\n",
    "        \n",
    "        h_forward = h_n[0]  \n",
    "        h_backward = h_n[1]  \n",
    "        sentence_rep = torch.cat([h_forward, h_backward], dim=1)  \n",
    "        return sentence_rep\n",
    "\n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, hidden_dim=512, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(4 * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        u = self.encoder(premise)\n",
    "        v = self.encoder(hypothesis)\n",
    "        combined = combine(u, v)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "hidden_dim = 300\n",
    "model = NLIClassifier(SentenceEncoder(embedding_layer, hidden_dim), input_dim=2 * hidden_dim)\n",
    "model.load_state_dict(torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/v2/snli_v2_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(premise_padded, hypothesis_padded)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "print(f\"BiLSTM Prediction: {label_map[predicted_class]}\")\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_idx = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/word_to_idx.pt\")\n",
    "pad_idx = word_to_idx.get('<pad>', 0)\n",
    "unk_idx = word_to_idx.get('<unk>', 1)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def encode(tokens):\n",
    "    return [word_to_idx.get(t, unk_idx) for t in tokens]\n",
    "\n",
    "\n",
    "premise_tokens = preprocess_text(premise_text)\n",
    "hypothesis_tokens = preprocess_text(hypothesis_text)\n",
    "premise_encoded = torch.tensor(encode(premise_tokens))\n",
    "hypothesis_encoded = torch.tensor(encode(hypothesis_tokens))\n",
    "\n",
    "\n",
    "premise_padded = pad_sequence([premise_encoded], batch_first=True, padding_value=pad_idx)\n",
    "hypothesis_padded = pad_sequence([hypothesis_encoded], batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "embedding_matrix = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/embedding_matrix.pt\", weights_only=False)\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True, padding_idx=pad_idx)\n",
    "\n",
    "def combine(u, v):\n",
    "    return torch.cat([u, v, torch.abs(u - v), u * v], dim=1)\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim=300):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_layer.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  \n",
    "        lengths = (x != 0).sum(dim=1).clamp(min=1).cpu()\n",
    "\n",
    "        \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        \n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        sentence_rep, _ = torch.max(lstm_out, dim=1)  \n",
    "        return sentence_rep\n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, hidden_dim=512, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(4 * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        u = self.encoder(premise)\n",
    "        v = self.encoder(hypothesis)\n",
    "        combined = combine(u, v)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "hidden_dim = 300\n",
    "model = NLIClassifier(SentenceEncoder(embedding_layer, hidden_dim), input_dim=2 * hidden_dim)\n",
    "model.load_state_dict(torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/v3/snli_v3_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(premise_padded, hypothesis_padded)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "print(f\"Bi LSTM with max poolong Prediction: {label_map[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As highlighted, all the LSTM based models predicted contradiction, whereas baseline predicts entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise_text = \"A man is walking a dog\"\n",
    "hypothesis_text = \"No cat is outside\"\n",
    "label = 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Prediction: entailment\n",
      "Uni LSTM Prediction: contradiction\n",
      "BiLSTM Prediction: contradiction\n",
      "Bi LSTM with max poolong Prediction: contradiction\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_idx = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/word_to_idx.pt\")\n",
    "pad_idx = word_to_idx.get('<pad>', 0)\n",
    "unk_idx = word_to_idx.get('<unk>', 1)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def encode(tokens):\n",
    "    return [word_to_idx.get(t, unk_idx) for t in tokens]\n",
    "\n",
    "\n",
    "premise_tokens = preprocess_text(premise_text)\n",
    "hypothesis_tokens = preprocess_text(hypothesis_text)\n",
    "premise_encoded = torch.tensor(encode(premise_tokens))\n",
    "hypothesis_encoded = torch.tensor(encode(hypothesis_tokens))\n",
    "\n",
    "premise_padded = pad_sequence([premise_encoded], batch_first=True, padding_value=pad_idx)\n",
    "hypothesis_padded = pad_sequence([hypothesis_encoded], batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "embedding_matrix = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/embedding_matrix.pt\", weights_only=False)\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True, padding_idx=pad_idx)\n",
    "\n",
    "def combine(u, v):\n",
    "    return torch.cat([u, v, torch.abs(u - v), u * v], dim=1)\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        mask = (x != pad_idx).unsqueeze(-1).float()\n",
    "        summed = torch.sum(embedded * mask, dim=1)\n",
    "        lengths = torch.sum(mask, dim=1)\n",
    "        return summed / lengths.clamp(min=1)\n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, hidden_dim=512, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(4 * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        u = self.encoder(premise)\n",
    "        v = self.encoder(hypothesis)\n",
    "        combined = combine(u, v)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "hidden_dim = 300\n",
    "model = NLIClassifier(SentenceEncoder(embedding_layer), input_dim=hidden_dim)\n",
    "model.load_state_dict(torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/Baseline/snli_baseline_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(premise_padded, hypothesis_padded)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "print(f\"Baseline Prediction: {label_map[predicted_class]}\")\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_idx = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/word_to_idx.pt\")\n",
    "pad_idx = word_to_idx.get('<pad>', 0)\n",
    "unk_idx = word_to_idx.get('<unk>', 1)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def encode(tokens):\n",
    "    return [word_to_idx.get(t, unk_idx) for t in tokens]\n",
    "\n",
    "\n",
    "premise_tokens = preprocess_text(premise_text)\n",
    "hypothesis_tokens = preprocess_text(hypothesis_text)\n",
    "premise_encoded = torch.tensor(encode(premise_tokens))\n",
    "hypothesis_encoded = torch.tensor(encode(hypothesis_tokens))\n",
    "\n",
    "\n",
    "premise_padded = pad_sequence([premise_encoded], batch_first=True, padding_value=pad_idx)\n",
    "hypothesis_padded = pad_sequence([hypothesis_encoded], batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "embedding_matrix = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/embedding_matrix.pt\", weights_only=False)\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True, padding_idx=pad_idx)\n",
    "\n",
    "def combine(u, v):\n",
    "    return torch.cat([u, v, torch.abs(u - v), u * v], dim=1)\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim=300):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_layer.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  \n",
    "        lengths = (x != 0).sum(dim=1).clamp(min=1).cpu()  \n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        _, (h_n, _) = self.lstm(packed)  \n",
    "        return h_n.squeeze(0) \n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, hidden_dim=512, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(4 * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        u = self.encoder(premise)\n",
    "        v = self.encoder(hypothesis)\n",
    "        combined = combine(u, v)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "hidden_dim = 300\n",
    "model = NLIClassifier(SentenceEncoder(embedding_layer), input_dim=hidden_dim)\n",
    "model.load_state_dict(torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/v1/snli_v1_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(premise_padded, hypothesis_padded)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "print(f\"Uni LSTM Prediction: {label_map[predicted_class]}\")\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_idx = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/word_to_idx.pt\")\n",
    "pad_idx = word_to_idx.get('<pad>', 0)\n",
    "unk_idx = word_to_idx.get('<unk>', 1)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def encode(tokens):\n",
    "    return [word_to_idx.get(t, unk_idx) for t in tokens]\n",
    "\n",
    "\n",
    "premise_tokens = preprocess_text(premise_text)\n",
    "hypothesis_tokens = preprocess_text(hypothesis_text)\n",
    "premise_encoded = torch.tensor(encode(premise_tokens))\n",
    "hypothesis_encoded = torch.tensor(encode(hypothesis_tokens))\n",
    "\n",
    "\n",
    "premise_padded = pad_sequence([premise_encoded], batch_first=True, padding_value=pad_idx)\n",
    "hypothesis_padded = pad_sequence([hypothesis_encoded], batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "embedding_matrix = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/embedding_matrix.pt\", weights_only=False)\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True, padding_idx=pad_idx)\n",
    "\n",
    "def combine(u, v):\n",
    "    return torch.cat([u, v, torch.abs(u - v), u * v], dim=1)\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim=300):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_layer.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  \n",
    "        lengths = (x != 0).sum(dim=1).clamp(min=1).cpu()\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        _, (h_n, _) = self.lstm(packed)  \n",
    "\n",
    "        \n",
    "        h_forward = h_n[0]  \n",
    "        h_backward = h_n[1]  \n",
    "        sentence_rep = torch.cat([h_forward, h_backward], dim=1)  \n",
    "        return sentence_rep\n",
    "\n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, hidden_dim=512, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(4 * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        u = self.encoder(premise)\n",
    "        v = self.encoder(hypothesis)\n",
    "        combined = combine(u, v)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "hidden_dim = 300\n",
    "model = NLIClassifier(SentenceEncoder(embedding_layer, hidden_dim), input_dim=2 * hidden_dim)\n",
    "model.load_state_dict(torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/v2/snli_v2_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(premise_padded, hypothesis_padded)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "print(f\"BiLSTM Prediction: {label_map[predicted_class]}\")\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "word_to_idx = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/word_to_idx.pt\")\n",
    "pad_idx = word_to_idx.get('<pad>', 0)\n",
    "unk_idx = word_to_idx.get('<unk>', 1)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def encode(tokens):\n",
    "    return [word_to_idx.get(t, unk_idx) for t in tokens]\n",
    "\n",
    "\n",
    "premise_tokens = preprocess_text(premise_text)\n",
    "hypothesis_tokens = preprocess_text(hypothesis_text)\n",
    "premise_encoded = torch.tensor(encode(premise_tokens))\n",
    "hypothesis_encoded = torch.tensor(encode(hypothesis_tokens))\n",
    "\n",
    "\n",
    "premise_padded = pad_sequence([premise_encoded], batch_first=True, padding_value=pad_idx)\n",
    "hypothesis_padded = pad_sequence([hypothesis_encoded], batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "\n",
    "embedding_matrix = torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/embedding_matrix.pt\", weights_only=False)\n",
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float), freeze=True, padding_idx=pad_idx)\n",
    "\n",
    "def combine(u, v):\n",
    "    return torch.cat([u, v, torch.abs(u - v), u * v], dim=1)\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim=300):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_layer.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  \n",
    "        lengths = (x != 0).sum(dim=1).clamp(min=1).cpu()\n",
    "\n",
    "        \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        \n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        sentence_rep, _ = torch.max(lstm_out, dim=1)  \n",
    "        return sentence_rep\n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, encoder, input_dim, hidden_dim=512, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(4 * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, premise, hypothesis):\n",
    "        u = self.encoder(premise)\n",
    "        v = self.encoder(hypothesis)\n",
    "        combined = combine(u, v)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "hidden_dim = 300\n",
    "model = NLIClassifier(SentenceEncoder(embedding_layer, hidden_dim), input_dim=2 * hidden_dim)\n",
    "model.load_state_dict(torch.load(\"/Users/sohamchatterjee/Documents/UvA/ATCS/Practical_1/Checkpoints/chatty_atcs/v3/snli_v3_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(premise_padded, hypothesis_padded)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "\n",
    "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "print(f\"Bi LSTM with max poolong Prediction: {label_map[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As highlighted, all the LSTM based models predicted contradiction, whereas baseline predicts entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # In the first example, the models predicted Contradiction likely due to the contradictory nature of the phrase \"Two men sitting\" and \"Nobody is sitting\". The models missed the representation for sun and shade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # In the second example, the models predicted Contradiction likely due to the contradictory nature of the phrase \"A man is walking\" and \"No cat is outside\". The models thought that walking is a thing usually done outside, but the hypothesis highlights that no cat is outside. Model failed to identify the difference between a dog being walked outside vs no cat present outside. Model assumes a negative correlation/contradiction, whereas in reality, the premise and hypothesis are completely independent of each other and hence neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
